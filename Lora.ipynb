{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b51ede8-67a2-44a0-9053-242a6a257242",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bb2ec81-20df-467e-9b25-25b7cf95a875",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLoRA(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, rank, alpha, merge=True, dropout=None):\n",
    "        super(LinearLoRA, self).__init__()\n",
    "        \n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.merge = merge\n",
    "\n",
    "        assert self.rank>0, \"The rank is not positive\"\n",
    "        self.linear = nn.Linear(in_dim, out_dim)\n",
    "        self.linear.weight.requires_grad = False\n",
    "        # keep note that the order of dimension should be reversed since F.linear asks the output_dim to be the first\n",
    "        self.lora_a = nn.Parameter(torch.empty(rank, in_dim))\n",
    "        self.lora_b = nn.Parameter(torch.empty(out_dim, rank))\n",
    "        self.scale = self.alpha/self.rank\n",
    "        if dropout:\n",
    "            self.dropout = nn.Dropout(p=dropout)\n",
    "        else:\n",
    "            self.dropout = nn.Identity()\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        nn.init.normal_(self.lora_a, mean=0.0, std=1.0)\n",
    "        nn.init.zeros_(self.lora_b)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # linear model y = xA^T + b. A should be out_dim x in_dim\n",
    "        if self.merge:\n",
    "            x = F.linear(x, self.linear.weight + (self.lora_b @ self.lora_a * self.scale), self.linear.bias)\n",
    "        else:\n",
    "            x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc0d4506-2463-4011-bc03-8805608625cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = LinearLoRA(5, 10, 2, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc2c5cc4-c370-4248-9d60-31574e614264",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(20, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03af90f7-c318-43fa-b6a6-cbf4cd94b682",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68d0245c-9de1-4928-ab50-59417c67afff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 10])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a454ab4-bba8-46b8-b9fd-580d144f9b1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
