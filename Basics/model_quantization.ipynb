{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d20998f-0cff-4be5-9613-d5ac811d5959",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mingpt.model import GPT\n",
    "from mingpt.bpe import BPETokenizer\n",
    "from huggingface_hub import snapshot_download\n",
    "import torch\n",
    "import gc\n",
    "import os\n",
    "import timm\n",
    "from transformers import AutoConfig, AutoModel\n",
    "from transformers.modeling_utils import load_sharded_checkpoint\n",
    "\n",
    "from accelerate import Accelerator, init_empty_weights, load_checkpoint_in_model\n",
    "from accelerate.utils import BnbQuantizationConfig, load_and_quantize_model, release_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dee06856-36ab-428b-9756-effbe9d95546",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b3923e0-8c4e-4118-963c-4ef76f62144e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = GPT.get_default_config()\n",
    "model_config.model_type = 'gpt2-xl'\n",
    "model_config.vocab_size = 50257\n",
    "model_config.block_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c741e6aa-77d6-4fe0-a498-8c927aa47426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 1557.61M\n"
     ]
    }
   ],
   "source": [
    "with init_empty_weights():\n",
    "    empty_model = GPT(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb20ece8-64ca-4378-bfc2-5698e30ae75e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fa67cc9f8434b7a864b3ef2b2fb0dae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "weights_location = snapshot_download(repo_id='marcsun13/gpt2-xl-linear-sharded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e48f6405-c365-4d85-8450-9ec5ba6e54cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.5.attn.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.5.attn.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.5.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.5.ln_2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.5.mlp.c_fc.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.5.mlp.c_fc.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.5.mlp.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.5.mlp.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.6.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.6.ln_1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.6.attn.c_attn.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.6.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.6.attn.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.6.attn.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.6.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.6.ln_2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.6.mlp.c_fc.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.6.mlp.c_fc.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.6.mlp.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.6.mlp.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.7.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.7.ln_1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.7.attn.c_attn.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.7.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.7.attn.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.7.attn.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.7.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.7.ln_2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.7.mlp.c_fc.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.7.mlp.c_fc.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.7.mlp.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.7.mlp.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.8.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.8.ln_1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.8.attn.c_attn.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.8.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.8.attn.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.8.attn.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.8.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.8.ln_2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.8.mlp.c_fc.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.8.mlp.c_fc.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.8.mlp.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.8.mlp.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.9.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.9.ln_1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.9.attn.c_attn.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.9.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.9.attn.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.9.attn.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.9.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.9.ln_2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.9.mlp.c_fc.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.9.mlp.c_fc.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.9.mlp.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.9.mlp.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.10.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.10.ln_1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.10.attn.c_attn.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.10.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.10.attn.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.10.attn.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.10.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.10.ln_2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.10.mlp.c_fc.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.10.mlp.c_fc.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.10.mlp.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.10.mlp.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.11.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.11.ln_1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.11.attn.c_attn.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.11.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.11.attn.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.11.attn.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.11.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.11.ln_2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.11.mlp.c_fc.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.11.mlp.c_fc.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.11.mlp.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.11.mlp.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.12.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.12.ln_1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.12.attn.c_attn.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.12.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.12.attn.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.12.attn.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.12.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.12.ln_2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.12.mlp.c_fc.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.12.mlp.c_fc.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.12.mlp.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.12.mlp.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.13.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.13.ln_1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.13.attn.c_attn.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.13.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.13.attn.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.13.attn.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.13.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.13.ln_2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.13.mlp.c_fc.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.13.mlp.c_fc.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.13.mlp.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.13.mlp.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.14.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.14.ln_1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.14.attn.c_attn.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.14.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.14.attn.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.14.attn.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.14.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.14.ln_2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.14.mlp.c_fc.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.14.mlp.c_fc.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.14.mlp.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.14.mlp.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.15.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.15.ln_1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.15.attn.c_attn.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.15.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.15.attn.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.15.attn.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.15.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.15.ln_2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.15.mlp.c_fc.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.15.mlp.c_fc.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.15.mlp.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.15.mlp.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.16.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.16.ln_1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.16.attn.c_attn.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.16.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.16.attn.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.16.attn.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.16.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.16.ln_2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.16.mlp.c_fc.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.16.mlp.c_fc.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.16.mlp.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.16.mlp.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.17.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.17.ln_1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.17.attn.c_attn.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.17.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.17.attn.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.17.attn.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.17.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.17.ln_2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.17.mlp.c_fc.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.17.mlp.c_fc.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.17.mlp.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.17.mlp.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.18.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.18.ln_1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.18.attn.c_attn.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.18.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.18.attn.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.18.attn.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.18.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.18.ln_2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.18.mlp.c_fc.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.18.mlp.c_fc.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.18.mlp.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.18.mlp.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.19.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.19.ln_1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.19.attn.c_attn.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.19.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.19.attn.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.19.attn.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.19.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.19.ln_2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.19.mlp.c_fc.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.19.mlp.c_fc.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.19.mlp.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.19.mlp.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.20.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.20.ln_1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.20.attn.c_attn.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.20.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.20.attn.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.20.attn.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.20.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.20.ln_2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.20.mlp.c_fc.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.20.mlp.c_fc.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.20.mlp.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.20.mlp.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.21.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.21.ln_1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.21.attn.c_attn.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.21.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.21.attn.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.21.attn.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.21.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.21.ln_2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.21.mlp.c_fc.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.21.mlp.c_fc.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.21.mlp.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.21.mlp.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.22.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.22.ln_1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.22.attn.c_attn.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.22.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.22.attn.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.22.attn.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.22.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.22.ln_2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.22.mlp.c_fc.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.22.mlp.c_fc.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.22.mlp.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.22.mlp.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.23.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.23.ln_1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.23.attn.c_attn.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.23.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.23.attn.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.23.attn.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.23.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.23.ln_2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.23.mlp.c_fc.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.23.mlp.c_fc.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.23.mlp.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.23.mlp.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.24.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.24.ln_1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.24.attn.c_attn.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.24.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.24.attn.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.24.attn.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.24.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.24.ln_2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.24.mlp.c_fc.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.24.mlp.c_fc.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.24.mlp.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.24.mlp.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.25.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.25.ln_1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.25.attn.c_attn.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.25.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.25.attn.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.25.attn.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.25.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.25.ln_2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.25.mlp.c_fc.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.25.mlp.c_fc.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.25.mlp.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.25.mlp.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.26.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.26.ln_1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.26.attn.c_attn.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.26.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.26.attn.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.26.attn.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.26.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.26.ln_2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.26.mlp.c_fc.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.26.mlp.c_fc.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.26.mlp.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.26.mlp.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.27.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.27.ln_1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.27.attn.c_attn.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.27.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.27.attn.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.27.attn.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.27.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.27.ln_2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.27.mlp.c_fc.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.27.mlp.c_fc.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.27.mlp.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.27.mlp.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.28.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.28.ln_1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.28.attn.c_attn.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.28.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.28.attn.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.28.attn.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.28.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.28.ln_2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.36.attn.c_attn.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.36.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.36.attn.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.36.attn.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.36.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.36.ln_2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.36.mlp.c_fc.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.36.mlp.c_fc.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.36.mlp.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.36.mlp.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.37.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.37.ln_1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.37.attn.c_attn.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.37.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.37.attn.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.37.attn.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.37.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.37.ln_2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.37.mlp.c_fc.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.37.mlp.c_fc.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.37.mlp.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.37.mlp.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.38.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.38.ln_1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.38.attn.c_attn.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.38.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.38.attn.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.38.attn.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.38.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.38.ln_2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.38.mlp.c_fc.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.38.mlp.c_fc.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.38.mlp.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.38.mlp.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.39.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.39.ln_1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.39.attn.c_attn.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.39.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.39.attn.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.39.attn.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.39.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.39.ln_2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.39.mlp.c_fc.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.39.mlp.c_fc.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.39.mlp.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.39.mlp.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.40.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.40.ln_1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.40.attn.c_attn.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.40.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.40.attn.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.40.attn.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.40.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.40.ln_2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.40.mlp.c_fc.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.40.mlp.c_fc.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.40.mlp.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.40.mlp.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.41.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.41.ln_1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.41.attn.c_attn.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.41.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.41.attn.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.41.attn.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.41.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.41.ln_2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.41.mlp.c_fc.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.41.mlp.c_fc.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.41.mlp.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.41.mlp.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.42.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.42.ln_1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.42.attn.c_attn.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.42.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.42.attn.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.42.attn.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.42.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.42.ln_2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.42.mlp.c_fc.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.42.mlp.c_fc.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.42.mlp.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.42.mlp.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.43.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.43.ln_1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.43.attn.c_attn.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.43.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.43.attn.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.43.attn.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.43.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.43.ln_2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.43.mlp.c_fc.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.43.mlp.c_fc.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.28.mlp.c_fc.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.28.mlp.c_fc.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.28.mlp.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.28.mlp.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.29.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.29.ln_1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.29.attn.c_attn.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.29.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.29.attn.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.29.attn.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.29.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.29.ln_2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.29.mlp.c_fc.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.29.mlp.c_fc.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.29.mlp.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.29.mlp.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.30.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.30.ln_1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.30.attn.c_attn.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.30.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.30.attn.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.30.attn.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.30.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.30.ln_2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.30.mlp.c_fc.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.30.mlp.c_fc.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.30.mlp.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.30.mlp.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.31.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.31.ln_1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.31.attn.c_attn.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.31.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.31.attn.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.31.attn.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.31.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.31.ln_2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.31.mlp.c_fc.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.31.mlp.c_fc.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.31.mlp.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.31.mlp.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.32.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.32.ln_1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.32.attn.c_attn.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.32.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.32.attn.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.32.attn.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.32.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.32.ln_2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.32.mlp.c_fc.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.32.mlp.c_fc.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.32.mlp.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.32.mlp.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.33.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.33.ln_1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.33.attn.c_attn.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.33.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.33.attn.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.33.attn.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.33.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.33.ln_2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.33.mlp.c_fc.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.33.mlp.c_fc.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.33.mlp.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.33.mlp.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.34.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.34.ln_1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.34.attn.c_attn.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.34.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.34.attn.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.34.attn.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.34.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.34.ln_2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.34.mlp.c_fc.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.34.mlp.c_fc.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.34.mlp.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.34.mlp.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.35.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.35.ln_1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.35.attn.c_attn.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.35.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.35.attn.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.35.attn.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.35.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.35.ln_2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.35.mlp.c_fc.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.35.mlp.c_fc.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.35.mlp.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.35.mlp.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.36.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.36.ln_1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.wte.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.wpe.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.0.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.0.ln_1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.0.attn.c_attn.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.0.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.0.attn.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.0.attn.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.0.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.0.ln_2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.0.mlp.c_fc.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.0.mlp.c_fc.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.0.mlp.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.0.mlp.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.1.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.1.ln_1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.1.attn.c_attn.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.1.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.1.attn.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.1.attn.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.1.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.1.ln_2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.1.mlp.c_fc.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.1.mlp.c_fc.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.1.mlp.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.1.mlp.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.2.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.2.ln_1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.2.attn.c_attn.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.2.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.2.attn.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.2.attn.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.2.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.2.ln_2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.2.mlp.c_fc.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.2.mlp.c_fc.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.2.mlp.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.2.mlp.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.3.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.3.ln_1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.3.attn.c_attn.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.3.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.3.attn.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.3.attn.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.3.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.3.ln_2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.3.mlp.c_fc.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.3.mlp.c_fc.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.3.mlp.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.3.mlp.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.4.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.4.ln_1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.4.attn.c_attn.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.4.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.4.attn.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.4.attn.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.4.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.4.ln_2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.4.mlp.c_fc.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.4.mlp.c_fc.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.4.mlp.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.4.mlp.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.5.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.5.ln_1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.5.attn.c_attn.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.5.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.43.mlp.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.43.mlp.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.44.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.44.ln_1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.44.attn.c_attn.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.44.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.44.attn.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.44.attn.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.44.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.44.ln_2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.44.mlp.c_fc.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.44.mlp.c_fc.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.44.mlp.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.44.mlp.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.45.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.45.ln_1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.45.attn.c_attn.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.45.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.45.attn.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.45.attn.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.45.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.45.ln_2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.45.mlp.c_fc.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.45.mlp.c_fc.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.45.mlp.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.45.mlp.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.46.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.46.ln_1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.46.attn.c_attn.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.46.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.46.attn.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.46.attn.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.46.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.46.ln_2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.46.mlp.c_fc.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.46.mlp.c_fc.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.46.mlp.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.46.mlp.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.47.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.47.ln_1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.47.attn.c_attn.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.47.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.47.attn.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.47.attn.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.47.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.47.ln_2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.47.mlp.c_fc.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.47.mlp.c_fc.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.47.mlp.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.h.47.mlp.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.ln_f.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.ln_f.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for lm_head.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_sharded_checkpoint(empty_model, weights_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "899b9164-6891-4d63-af69-6862dad17c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_model_size(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total number of parameters: {total_params}\")\n",
    "    \n",
    "    total_size = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "    total_size_mb = total_size / (1024 ** 2)\n",
    "    total_size_gb = total_size / (1024 ** 3)\n",
    "    \n",
    "    print(f\"Total model size: {total_size_mb:.2f} MB\")\n",
    "    print(f\"Total model size: {total_size_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7297f18-12ef-4cc8-8ef4-6398141ae5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 1638022400\n",
      "Total model size: 6248.56 MB\n",
      "Total model size: 6.10 GB\n"
     ]
    }
   ],
   "source": [
    "check_model_size(empty_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c4463be-11d8-4ea3-b9ec-76771affb3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_quantization_config = BnbQuantizationConfig(load_in_8bit=True, llm_int8_threshold=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b12dda5-6496-411f-bf7b-18a02656ab88",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = load_and_quantize_model(empty_model, weights_location=weights_location, bnb_quantization_config=bnb_quantization_config, device_map = \"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da3740a-828c-427b-aecb-15134385a723",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71f02d40-5069-4f5f-afb2-097368ed13fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_weights_location = \"quantized_models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689159f4-535a-48db-a1cc-4ca82de4f99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator.save_model(quantized_model, new_weights_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1d65227-bdb9-4f0f-b05c-eca67c2579c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at quantized_models were not used when initializing GPT: {'transformer.h.45.mlp.c_proj.weight_format', 'transformer.h.9.attn.c_proj.weight_format', 'transformer.h.8.attn.c_attn.weight_format', 'transformer.h.5.mlp.c_proj.weight_format', 'transformer.h.10.mlp.c_proj.weight_format', 'transformer.h.33.attn.c_attn.weight_format', 'transformer.h.18.mlp.c_proj.weight_format', 'transformer.h.33.mlp.c_proj.weight_format', 'transformer.h.10.attn.c_attn.weight_format', 'transformer.h.40.attn.c_attn.weight_format', 'transformer.h.30.mlp.c_fc.weight_format', 'transformer.h.42.mlp.c_proj.weight_format', 'transformer.h.10.mlp.c_fc.weight_format', 'transformer.h.24.attn.c_attn.weight_format', 'transformer.h.16.attn.c_proj.weight_format', 'transformer.h.20.attn.c_proj.weight_format', 'transformer.h.32.attn.c_attn.weight_format', 'transformer.h.28.mlp.c_proj.weight_format', 'transformer.h.23.attn.c_attn.weight_format', 'transformer.h.14.attn.c_attn.weight_format', 'transformer.h.20.mlp.c_fc.weight_format', 'transformer.h.12.attn.c_proj.weight_format', 'transformer.h.4.attn.c_proj.weight_format', 'transformer.h.36.mlp.c_fc.weight_format', 'transformer.h.46.mlp.c_proj.weight_format', 'transformer.h.35.mlp.c_fc.weight_format', 'transformer.h.43.mlp.c_proj.weight_format', 'transformer.h.8.attn.c_proj.weight_format', 'transformer.h.21.mlp.c_proj.weight_format', 'transformer.h.18.attn.c_proj.weight_format', 'transformer.h.41.mlp.c_proj.weight_format', 'transformer.h.30.attn.c_attn.weight_format', 'transformer.h.38.mlp.c_fc.weight_format', 'transformer.h.31.mlp.c_fc.weight_format', 'transformer.h.20.mlp.c_proj.weight_format', 'transformer.h.30.attn.c_proj.weight_format', 'transformer.h.0.mlp.c_proj.weight_format', 'transformer.h.36.attn.c_attn.weight_format', 'transformer.h.39.mlp.c_proj.weight_format', 'transformer.h.16.attn.c_attn.weight_format', 'transformer.h.17.mlp.c_fc.weight_format', 'transformer.h.47.mlp.c_proj.weight_format', 'transformer.h.5.mlp.c_fc.weight_format', 'transformer.h.15.attn.c_proj.weight_format', 'transformer.h.22.mlp.c_proj.weight_format', 'transformer.h.34.attn.c_attn.weight_format', 'transformer.h.18.mlp.c_fc.weight_format', 'transformer.h.13.attn.c_attn.weight_format', 'transformer.h.16.mlp.c_proj.weight_format', 'transformer.h.0.mlp.c_fc.weight_format', 'transformer.h.1.attn.c_proj.weight_format', 'transformer.h.36.mlp.c_proj.weight_format', 'transformer.h.7.mlp.c_fc.weight_format', 'transformer.h.1.mlp.c_fc.weight_format', 'transformer.h.0.attn.c_proj.weight_format', 'transformer.h.28.mlp.c_fc.weight_format', 'transformer.h.24.mlp.c_fc.weight_format', 'transformer.h.30.mlp.c_proj.weight_format', 'transformer.h.11.attn.c_proj.weight_format', 'transformer.h.46.attn.c_proj.weight_format', 'transformer.h.19.mlp.c_proj.weight_format', 'transformer.h.13.mlp.c_proj.weight_format', 'transformer.h.31.mlp.c_proj.weight_format', 'transformer.h.37.attn.c_proj.weight_format', 'transformer.h.17.mlp.c_proj.weight_format', 'transformer.h.32.attn.c_proj.weight_format', 'transformer.h.41.attn.c_proj.weight_format', 'transformer.h.42.mlp.c_fc.weight_format', 'transformer.h.29.mlp.c_fc.weight_format', 'transformer.h.35.attn.c_proj.weight_format', 'transformer.h.6.attn.c_proj.weight_format', 'transformer.h.47.mlp.c_fc.weight_format', 'transformer.h.10.attn.c_proj.weight_format', 'transformer.h.26.mlp.c_proj.weight_format', 'transformer.h.9.attn.c_attn.weight_format', 'transformer.h.39.attn.c_proj.weight_format', 'transformer.h.16.mlp.c_fc.weight_format', 'transformer.h.47.attn.c_proj.weight_format', 'transformer.h.11.mlp.c_fc.weight_format', 'transformer.h.27.mlp.c_fc.weight_format', 'transformer.h.41.mlp.c_fc.weight_format', 'transformer.h.2.attn.c_proj.weight_format', 'transformer.h.13.mlp.c_fc.weight_format', 'transformer.h.13.attn.c_proj.weight_format', 'transformer.h.40.attn.c_proj.weight_format', 'transformer.h.29.attn.c_proj.weight_format', 'transformer.h.9.mlp.c_fc.weight_format', 'transformer.h.45.mlp.c_fc.weight_format', 'transformer.h.25.attn.c_proj.weight_format', 'transformer.h.24.attn.c_proj.weight_format', 'transformer.h.24.mlp.c_proj.weight_format', 'transformer.h.4.mlp.c_fc.weight_format', 'transformer.h.43.attn.c_attn.weight_format', 'transformer.h.31.attn.c_attn.weight_format', 'transformer.h.4.mlp.c_proj.weight_format', 'transformer.h.4.attn.c_attn.weight_format', 'transformer.h.42.attn.c_proj.weight_format', 'transformer.h.12.attn.c_attn.weight_format', 'transformer.h.33.attn.c_proj.weight_format', 'transformer.h.6.attn.c_attn.weight_format', 'transformer.h.2.mlp.c_proj.weight_format', 'transformer.h.17.attn.c_proj.weight_format', 'transformer.h.37.mlp.c_fc.weight_format', 'transformer.h.19.attn.c_proj.weight_format', 'transformer.h.32.mlp.c_proj.weight_format', 'transformer.h.14.attn.c_proj.weight_format', 'transformer.h.23.mlp.c_proj.weight_format', 'transformer.h.26.attn.c_attn.weight_format', 'transformer.h.12.mlp.c_fc.weight_format', 'transformer.h.19.mlp.c_fc.weight_format', 'transformer.h.28.attn.c_attn.weight_format', 'transformer.h.37.mlp.c_proj.weight_format', 'transformer.h.40.mlp.c_fc.weight_format', 'transformer.h.15.mlp.c_fc.weight_format', 'transformer.h.1.mlp.c_proj.weight_format', 'transformer.h.0.attn.c_attn.weight_format', 'transformer.h.7.attn.c_attn.weight_format', 'transformer.h.45.attn.c_attn.weight_format', 'transformer.h.1.attn.c_attn.weight_format', 'transformer.h.44.attn.c_attn.weight_format', 'transformer.h.27.mlp.c_proj.weight_format', 'transformer.h.2.attn.c_attn.weight_format', 'transformer.h.44.attn.c_proj.weight_format', 'transformer.h.11.mlp.c_proj.weight_format', 'transformer.h.38.mlp.c_proj.weight_format', 'transformer.h.9.mlp.c_proj.weight_format', 'transformer.h.12.mlp.c_proj.weight_format', 'transformer.h.23.mlp.c_fc.weight_format', 'transformer.h.21.attn.c_attn.weight_format', 'transformer.h.26.mlp.c_fc.weight_format', 'transformer.h.8.mlp.c_fc.weight_format', 'transformer.h.47.attn.c_attn.weight_format', 'transformer.h.45.attn.c_proj.weight_format', 'transformer.h.7.mlp.c_proj.weight_format', 'transformer.h.11.attn.c_attn.weight_format', 'transformer.h.25.mlp.c_proj.weight_format', 'transformer.h.34.mlp.c_fc.weight_format', 'transformer.h.8.mlp.c_proj.weight_format', 'transformer.h.15.attn.c_attn.weight_format', 'transformer.h.46.attn.c_attn.weight_format', 'transformer.h.35.mlp.c_proj.weight_format', 'transformer.h.43.attn.c_proj.weight_format', 'transformer.h.27.attn.c_attn.weight_format', 'transformer.h.31.attn.c_proj.weight_format', 'transformer.h.2.mlp.c_fc.weight_format', 'transformer.h.5.attn.c_proj.weight_format', 'transformer.h.15.mlp.c_proj.weight_format', 'transformer.h.14.mlp.c_fc.weight_format', 'transformer.h.17.attn.c_attn.weight_format', 'transformer.h.22.mlp.c_fc.weight_format', 'transformer.h.25.mlp.c_fc.weight_format', 'transformer.h.21.mlp.c_fc.weight_format', 'transformer.h.3.attn.c_attn.weight_format', 'transformer.h.3.attn.c_proj.weight_format', 'transformer.h.42.attn.c_attn.weight_format', 'transformer.h.23.attn.c_proj.weight_format', 'transformer.h.35.attn.c_attn.weight_format', 'transformer.h.21.attn.c_proj.weight_format', 'transformer.h.26.attn.c_proj.weight_format', 'transformer.h.39.attn.c_attn.weight_format', 'transformer.h.28.attn.c_proj.weight_format', 'transformer.h.44.mlp.c_fc.weight_format', 'transformer.h.5.attn.c_attn.weight_format', 'transformer.h.29.attn.c_attn.weight_format', 'transformer.h.38.attn.c_attn.weight_format', 'transformer.h.14.mlp.c_proj.weight_format', 'transformer.h.6.mlp.c_fc.weight_format', 'transformer.h.34.mlp.c_proj.weight_format', 'transformer.h.18.attn.c_attn.weight_format', 'transformer.h.41.attn.c_attn.weight_format', 'transformer.h.22.attn.c_attn.weight_format', 'transformer.h.38.attn.c_proj.weight_format', 'transformer.h.22.attn.c_proj.weight_format', 'transformer.h.40.mlp.c_proj.weight_format', 'transformer.h.3.mlp.c_fc.weight_format', 'transformer.h.6.mlp.c_proj.weight_format', 'transformer.h.20.attn.c_attn.weight_format', 'transformer.h.29.mlp.c_proj.weight_format', 'transformer.h.37.attn.c_attn.weight_format', 'transformer.h.27.attn.c_proj.weight_format', 'transformer.h.39.mlp.c_fc.weight_format', 'transformer.h.34.attn.c_proj.weight_format', 'transformer.h.43.mlp.c_fc.weight_format', 'transformer.h.7.attn.c_proj.weight_format', 'transformer.h.25.attn.c_attn.weight_format', 'transformer.h.46.mlp.c_fc.weight_format', 'transformer.h.44.mlp.c_proj.weight_format', 'transformer.h.19.attn.c_attn.weight_format', 'transformer.h.36.attn.c_proj.weight_format', 'transformer.h.3.mlp.c_proj.weight_format', 'transformer.h.33.mlp.c_fc.weight_format', 'transformer.h.32.mlp.c_fc.weight_format'}. This may or may not be an issue - make sure that the checkpoint does not have unnecessary parameters, or that the model definition correctly corresponds to the checkpoint.\n"
     ]
    }
   ],
   "source": [
    "quantized_model_from_saved = load_and_quantize_model(empty_model, weights_location=new_weights_location, bnb_quantization_config=bnb_quantization_config, device_map = \"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff69d853-924c-412c-947a-02f8b8fedb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 1638022400\n",
      "Total model size: 1718.03 MB\n",
      "Total model size: 1.68 GB\n"
     ]
    }
   ],
   "source": [
    "check_model_size(quantized_model_from_saved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe847233-0756-45fa-9f5b-9a6366bfaf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "release_memory(quantized_model_from_saved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537268e2-eadc-4d0a-9cf5-0b87c163eb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flush():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bcae8d-69c8-4cae-8087-7734be78c04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989d8ae9-9e33-41b0-a5d1-51af3e16d862",
   "metadata": {},
   "source": [
    "# Use the Quantized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7b6c753-252e-48ee-b6f0-ede24cb0495a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tokenizer\n",
    "prompt = \"Hello my name is\"\n",
    "tokenizer = BPETokenizer()\n",
    "x1 = tokenizer(prompt).to(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "100f555f-fd93-4591-8560-a898294666f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(50257, 1600)\n",
       "    (wpe): Embedding(1024, 1600)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-47): 48 x Block(\n",
       "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear8bitLt(in_features=1600, out_features=4800, bias=True)\n",
       "          (c_proj): Linear8bitLt(in_features=1600, out_features=1600, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModuleDict(\n",
       "          (c_fc): Linear8bitLt(in_features=1600, out_features=6400, bias=True)\n",
       "          (c_proj): Linear8bitLt(in_features=6400, out_features=1600, bias=True)\n",
       "          (act): NewGELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1600, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_model_from_saved.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6561ef9-20c6-440a-b414-f023bff84a2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "for parameter in quantized_model_from_saved.parameters():\n",
    "    print(parameter.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65b3abd9-1dfb-4233-9a22-3e4030f5e182",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = quantized_model_from_saved.generate(x1, max_new_tokens=10, do_sample=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4938a27b-186f-43b9-bb37-73b0ea91a154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello my name is John Doe, and I'm a big fan of\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(outputs.cpu().squeeze()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590614d3-0fb9-4747-9432-e3806f571a83",
   "metadata": {},
   "source": [
    "# Load in 4 bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28527241-65e6-48ae-8fa2-20c8361c28c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 1557.61M\n"
     ]
    }
   ],
   "source": [
    "model_config = GPT.get_default_config()\n",
    "model_config.model_type = 'gpt2-xl'\n",
    "model_config.vocab_size = 50257\n",
    "model_config.block_size = 1024\n",
    "\n",
    "with init_empty_weights():\n",
    "  empty_model = GPT(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4527218-09c9-4c4a-acb0-82c2b83b108b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get quantization config\n",
    "config = BnbQuantizationConfig(load_in_4bit=True,\n",
    "                               bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                               bnb_4bit_use_double_quant=True,\n",
    "                               bnb_4bit_quant_type=\"nf4\"\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12be406a-cab3-4dd3-a62c-22ccfa46a393",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4bit = load_and_quantize_model(empty_model,\n",
    "                                     bnb_quantization_config = config,\n",
    "                                     weights_location = weights_location,\n",
    "                                     device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "635c2cc7-e6d0-4002-9792-06106a98122d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT(\n",
      "  (transformer): ModuleDict(\n",
      "    (wte): Embedding(50257, 1600)\n",
      "    (wpe): Embedding(1024, 1600)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-47): 48 x Block(\n",
      "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_attn): Linear4bit(in_features=1600, out_features=4800, bias=True)\n",
      "          (c_proj): Linear4bit(in_features=1600, out_features=1600, bias=True)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): ModuleDict(\n",
      "          (c_fc): Linear4bit(in_features=1600, out_features=6400, bias=True)\n",
      "          (c_proj): Linear4bit(in_features=6400, out_features=1600, bias=True)\n",
      "          (act): NewGELU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1600, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_4bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3c2cbaf-713f-46d8-b0c0-c92fdaeb8762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello my name is John. I am a student at the University of\n"
     ]
    }
   ],
   "source": [
    "model_4bit.eval()\n",
    "outputs = model_4bit.generate(x1, max_new_tokens=10, do_sample=False)[0]\n",
    "print(tokenizer.decode(outputs.cpu().squeeze()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb16bd26-afdb-458b-b72e-bd0fe73a831b",
   "metadata": {},
   "source": [
    "# Train a quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4839b31c-67c3-40e3-95b3-0754d1427019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ba8bdbf61f3414b857f0ae99610a76f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89076513860f4ffcb584be1fae60bf7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00046.safetensors:   7%|6         | 63.6M/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "573017c5da3e4bf0951f750fa0969b92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee78f8de71dd41cb90bb796ec1d4e0ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78bddf9456014ad6acea9934b677008a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00007-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3e584f4e22143fbb174df30db207924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00008-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c33d1e91f97453b8d064912bf3279da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00009-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7e83ca9c3a8404998cb91b55883378b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00010-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f9ddc4498ff43839cbd211706e6a738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00011-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd11bc3a359c4b8e96e08668eeb4ddeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00012-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04c2455ada4b47d092233975d0c861f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00013-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d4d06dd0a72424d92492acd4120650f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00014-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cbe337994e741d99c24fc1fbe84301e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00015-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"EleutherAI/gpt-neox-20b\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c4418c-c18b-44b0-a2fd-3442b61e6578",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4455ffd2-73ff-42f2-8b39-cdd53c4ed60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c9276f-b2d9-41a0-b3ad-278344b6ca6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8, \n",
    "    lora_alpha=32, \n",
    "    target_modules=[\"query_key_value\"], \n",
    "    lora_dropout=0.05, \n",
    "    bias=\"none\", \n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
