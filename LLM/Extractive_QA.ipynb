{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d631ad6-9144-4a42-8d65-a06572460a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extractive: extract the answer from the given context.\n",
    "# Abstractive: generate an answer from the context that correctly answers the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bacfa1a-e1a3-4e11-a4d5-f82edeca1bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb723e15-767d-4fe6-986c-bc919018d593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "489d41eb783b408e9e61a88392e62f78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd3208ab-d43f-489d-b086-06a011d65911",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61276ce5-009c-474f-abd8-f3bd5b63d5b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19e47df0ce9d4cef95c304559f53b6f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcbf4c9b0518418f9acc3d64d1d8d680",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/14.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cb541b36c8444d3a7c35117dc0f2caf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.82M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1509d38e8f24bad973b83176ab24d7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16a5447d80f340f6b47f436d50f3e50f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "squad = load_dataset(\"squad\", split=\"train[:5000]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71c75079-a0a6-4a6e-b424-8f0f67b06fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "squad = squad.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d205183a-0ddd-455b-a51c-d0df75981bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '57340136d058e614000b6784',\n",
       " 'title': 'Genocide',\n",
       " 'context': \"Highlighting the potential for state and non-state actors to commit genocide in the 21st century, for example, in failed states or as non-state actors acquire weapons of mass destruction, Adrian Gallagher defined genocide as 'When a source of collective power (usually a state) intentionally uses its power base to implement a process of destruction in order to destroy a group (as defined by the perpetrator), in whole or in substantial part, dependent upon relative group size'. The definition upholds the centrality of intent, the multidimensional understanding of destroy, broadens the definition of group identity beyond that of the 1948 definition yet argues that a substantial part of a group has to be destroyed before it can be classified as genocide (dependent on relative group size).\",\n",
       " 'question': \"In Gallagher's definition of genocide, a source of what is malicious in it implementation of the destruction of a group? \",\n",
       " 'answers': {'text': ['collective power'], 'answer_start': [243]}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c4afdc4-8b03-455f-a003-b9dfddf24312",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d34967d-208a-49b2-832f-d2d19a7968df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/geshi/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00cc3706-3ad4-4b2c-9725-c19f21b6c80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=384,\n",
    "        truncation=\"only_second\",\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        answer = answers[i]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label it (0, 0)\n",
    "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d188150-29a2-456c-89ee-7df30861253a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad396bca4964476cab06f36b3c21bab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cecd349c5f87463181979db1afd352f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_squad = squad.map(preprocess_function, batched=True, remove_columns=squad[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4979fdde-df5b-44c7-9491-883a05add76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator, AutoModelForQuestionAnswering, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d8b6690-4216-4fe6-ba84-af0e4f6cd8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c12ff69c-5130-4068-b287-a8011c771f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert/distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "886847f8-4879-4dab-9daf-037ba199a1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"distllbert_qa_model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6219c166-4abf-45cf-8716-909506bc80a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_squad[\"train\"],\n",
    "    eval_dataset=tokenized_squad[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c97eed-22ba-4322-9808-86643e99f822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-08-30 12:28:22,148] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  Â·Â·Â·Â·Â·Â·Â·Â·\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/geshi/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/geshi/MLFromScratch/LLM/wandb/run-20240830_122839-i6k588y5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/geshijoker-uc-davis/huggingface/runs/i6k588y5' target=\"_blank\">distllbert_qa_model</a></strong> to <a href='https://wandb.ai/geshijoker-uc-davis/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/geshijoker-uc-davis/huggingface' target=\"_blank\">https://wandb.ai/geshijoker-uc-davis/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/geshijoker-uc-davis/huggingface/runs/i6k588y5' target=\"_blank\">https://wandb.ai/geshijoker-uc-davis/huggingface/runs/i6k588y5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='162' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [162/375 00:39 < 00:52, 4.07 it/s, Epoch 1.29/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.925725</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a58428-0773-42f8-8010-b81e54bd1359",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8ba7b8-9052-44f2-8313-0e73529afaf4",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ee04d54-2650-4e8f-bca9-aecbf3f59203",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How many programming languages does BLOOM support?\"\n",
    "context = \"BLOOM has 176 billion parameters and can generate text in 46 languages natural languages and 13 programming languages.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b1526a2-3a3a-48ae-88ed-e9492292a21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c1373d9-ddc5-4bf1-9538-76fde63d15a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "question_answerer = pipeline(\"question-answering\", model=\"geshijoker/distllbert_qa_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd000773-d94c-4c80-a935-b751d08e8bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.14677521586418152, 'start': 10, 'end': 21, 'answer': '176 billion'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_answerer(question=question, context=context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15eedaa-2ec9-4709-abb0-f0b64d085be6",
   "metadata": {},
   "source": [
    "# Inference with Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "490a826e-182e-4c69-8f0a-0fcb9e680f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10186a34-0090-45d1-9cd4-e79e92c5d53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"geshijoker/distllbert_qa_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8247ad6-3826-4fa1-8c7f-151bdeb5a926",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(question, context, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f61e86f6-175e-4d92-bc33-882a96c3673e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForQuestionAnswering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5b80d7a-975c-4f96-819b-e9c51cd9be17",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained(\"geshijoker/distllbert_qa_model\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bf9f93a-4689-42d4-9e9b-f8e7435000d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_start_index = outputs.start_logits.argmax()\n",
    "answer_end_index = outputs.end_logits.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41b6072f-3d2c-430f-86ec-09cb086291fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(12)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_start_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b00c9b04-615e-40ac-8509-adaaf95c660d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'176 billion'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
    "tokenizer.decode(predict_answer_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cc7ea8-5579-4c76-847d-f03fa2092efb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
