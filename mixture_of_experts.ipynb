{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee61ba6b-ff68-4e9b-bd1f-e8dd3ada33ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ac34c64-1675-4b6a-928e-31319b7b019c",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, N, D = 16, 512, 64\n",
    "vocab_size = 50012\n",
    "n_experts = 8\n",
    "n_heads = 8\n",
    "expansion = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5018da1-8101-41e4-9c39-c66f20000d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.rand(B, N, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b2812eb-a31c-4e48-a59e-b3136a5e7787",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpertsLayer(nn.Module):\n",
    "    def __init__(self, dim, expansion):\n",
    "        super(ExpertsLayer, self).__init__()\n",
    "        self.linear1 = nn.Linear(dim, dim*expansion)\n",
    "        self.relu = nn.GELU()\n",
    "        self.linear2 = nn.Linear(dim*expansion, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c8c1777-5c60-441d-9c7b-4c90bc98db4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 512, 64])\n"
     ]
    }
   ],
   "source": [
    "expert = ExpertsLayer(D, expansion=expansion)\n",
    "print(expert(inputs).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23d8ce5f-4bb5-4aff-9e31-8791581c73fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatingLayer(nn.Module):\n",
    "    def __init__(self, dim, n_experts):\n",
    "        super(GatingLayer, self).__init__()\n",
    "        self.gate = nn.Linear(dim, n_experts)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.softmax(self.gate(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36e84aff-5dd8-4d10-8c39-e86209c8689e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 512, 8])\n"
     ]
    }
   ],
   "source": [
    "gate = GatingLayer(D, n_experts)\n",
    "print(gate(inputs).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d1a943c-5854-4e8f-bc62-24e9e61d2c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoEModule(nn.Module):\n",
    "    def __init__(self, dim, n_experts, expansion):\n",
    "        super(MoEModule, self).__init__()\n",
    "        self.experts = nn.ModuleList([ExpertsLayer(dim, expansion) for _ in range(n_experts)])\n",
    "        self.gate = GatingLayer(dim, n_experts)\n",
    "\n",
    "    def forward(self, x):\n",
    "        experts_output = [expert(x) for expert in self.experts]\n",
    "        experts_prob = self.gate(x).unsqueeze(-2)\n",
    "        output = torch.stack([experts_prob[..., i]*expert_output[i] for i, expert_output in enumerate(experts_output)], dim=-1)\n",
    "        return output.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd01392c-5136-419e-bef9-e3113e93dcdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 512, 64])\n"
     ]
    }
   ],
   "source": [
    "moe = MoEModule(D, n_experts, expansion=expansion)\n",
    "print(moe(inputs).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "483efef4-7f81-43a7-9647-614abbf6746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoEDecoderBlock(nn.Module):\n",
    "    def __init__(self, dim, n_heads, n_experts, expansion):\n",
    "        super(MoEDecoderBlock, self).__init__()\n",
    "        self.multihead_attn = nn.MultiheadAttention(dim, n_heads)\n",
    "        self.norm1 = nn.LayerNorm([dim]) # LayerNorm with dropout or RMSNorm without dropout\n",
    "        self.moe = MoEModule(dim, n_experts, expansion)\n",
    "        self.norm2 = nn.LayerNorm([dim])\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output, attn_output_weights = self.multihead_attn(x, x, x)\n",
    "        att_out = self.norm1(attn_output + x)\n",
    "        moe_out = self.moe(att_out)\n",
    "        out = self.norm2(moe_out + x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03fd40df-00a1-43f6-9396-345d8b6d8c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 512, 64])\n"
     ]
    }
   ],
   "source": [
    "decoder = MoEDecoderBlock(D, n_heads, n_experts, expansion)\n",
    "print(decoder(inputs).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04a63e8-5a41-4837-9020-0e720bac151b",
   "metadata": {},
   "source": [
    "# Sparse MoE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e863a4b6-8710-4543-b4e6-cb643b9e90e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseMoEModule(nn.Module):\n",
    "    def __init__(self, dim, n_experts, expansion):\n",
    "        super(SparseMoEModule, self).__init__()\n",
    "        self.experts = nn.ModuleList([ExpertsLayer(dim, expansion) for _ in range(n_experts)])\n",
    "        self.gate = GatingLayer(dim, n_experts)\n",
    "\n",
    "    def forward(self, x, top_k):\n",
    "        experts_prob = self.gate(x)\n",
    "        topk_experts_prob, topk_indices = experts_prob.topk(top_k, dim=-1, sorted=False)\n",
    "        # Create a mask to zero out the contribution of non-topk experts\n",
    "        mask = torch.zeros_like(experts_prob).scatter_(2, topk_indices, 1)\n",
    "        # Use the mask to retain only the topk gating scores\n",
    "        experts_prob = F.normalize(experts_prob, p=1, dim=-1)\n",
    "\n",
    "        expert_outputs = torch.stack([expert(x) for expert in self.experts], dim=2)\n",
    "        output = torch.einsum('bte, bteo -> bto', experts_prob, expert_outputs)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1d3d3be-8dab-4d5f-ba47-d6ad6e0ed4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 512, 64])\n"
     ]
    }
   ],
   "source": [
    "smoe = SparseMoEModule(D, n_experts, expansion)\n",
    "print(smoe(inputs,3).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "65b2a7e7-c739-4f2e-b00e-3cd7766ad2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerWithSparseMoE(nn.Module): \n",
    "    # only with one MoE layer at the end but not MoE at all transformer blocks\n",
    "    def __init__(self, n_layers, dim, n_heads, n_experts, expasion, vocab_size):\n",
    "        super(TransformerWithSparseMoE, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, dim)\n",
    "        self.layers = nn.ModuleList([nn.TransformerEncoderLayer(d_model=dim, nhead=n_heads) for _ in range(n_layers)])\n",
    "        self.moe_layer = SparseMoEModule(dim, n_experts, expasion)\n",
    "        self.proj = nn.Linear(dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, top_k):\n",
    "        x = self.embedding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.moe_layer(x, top_k)\n",
    "        logits = self.proj(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e70506db-4240-4bb3-bd3f-508ff8743a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model with configurations matching Mixtral 8x7B\n",
    "model = TransformerWithSparseMoE(\n",
    "    n_layers=6,              # Number of transformer layers\n",
    "    dim=D,                   # Dimension of the model\n",
    "    n_heads=n_heads,         # Dimension of each head in the multi-head attention mechanisms\n",
    "    n_experts=n_experts,     # Number of experts in the MoE layer\n",
    "    expasion=expansion,      # Number of times of Expansion\n",
    "    vocab_size=vocab_size,   # Vocabulary size for the embedding layer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0db3c77f-d4fd-4d79-814d-3d966998eb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_inputs = torch.randint(0, vocab_size, size=(B, N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7dbf8a52-cb02-47c9-a942-ede5e7fc9188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 512, 50012])\n"
     ]
    }
   ],
   "source": [
    "print(model(tokenized_inputs, 3).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa2131e-3f62-4b60-9900-203a3a2be1b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
