{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c3162ba-f2f6-43ad-a188-ee1fde3b8f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fc57be1-42c2-4510-a35a-b81ae8b00635",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9b23143-941b-446f-aa03-9b3c2ef67f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://abdulkaderhelwan.medium.com/swiglu-activation-function-77627e0b2b52"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c4cbae-e4c2-48e4-af68-23277306d91e",
   "metadata": {},
   "source": [
    "# Tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a67d9a89-43f2-41cd-bb4b-2f05718074ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "767d052c-690c-45a6-9fe2-a52fa0899d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-6.7055225e-08 -5.9604645e-08  5.9604645e-08  5.9604645e-08\n",
      " -1.1175871e-08]\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5, requires_grad=False)\n",
    "y = nn.Tanh()(x).numpy()\n",
    "y_ = tanh(x.numpy())\n",
    "print(y-y_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da9ecf7-d2e7-4eb5-940b-5a2a940e8fae",
   "metadata": {},
   "source": [
    "# Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc4450b5-ed50-4db6-8735-cf34d7814249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1. / (1. + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42b6857e-9d8e-47d9-a802-8fb1f1aa1d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.9604645e-08 -2.9802322e-08  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00]\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5, requires_grad=False)\n",
    "y = nn.Sigmoid()(x).numpy()\n",
    "y_ = sigmoid(x.numpy())\n",
    "print(y-y_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e90246-741c-4579-b068-39d697b6e124",
   "metadata": {},
   "source": [
    "# SoftMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e5ef6f1-4a81-41c5-88ab-d4f58985afd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, axis=None):\n",
    "    return np.exp(x)/np.exp(x).sum(axis=axis, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1f84f4e-6d5d-40f4-b42e-16859ec8c914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.7252903e-09 -7.4505806e-09  5.9604645e-08]\n",
      " [ 0.0000000e+00  2.9802322e-08  0.0000000e+00]\n",
      " [ 0.0000000e+00  0.0000000e+00 -3.7252903e-09]\n",
      " [ 0.0000000e+00  2.9802322e-08 -2.9802322e-08]\n",
      " [ 0.0000000e+00  0.0000000e+00  2.9802322e-08]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/geshi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5, 3, requires_grad=False)\n",
    "y = nn.Softmax()(x).numpy()\n",
    "y_ = softmax(x.numpy(), axis=1)\n",
    "print(y-y_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452ee285-b283-4a2a-8ed2-b7079013bea9",
   "metadata": {},
   "source": [
    "# ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c28fd733-f88b-42b7-877a-cf513746658d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(x, np.zeros_like(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00623294-ed21-4b87-9920-4e0194ab30b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5, requires_grad=False)\n",
    "y = nn.ReLU()(x).numpy()\n",
    "y_ = relu(x.numpy())\n",
    "print(y-y_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22198923-ee59-42d3-92fd-9f242de6076f",
   "metadata": {},
   "source": [
    "# GLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf9bebed-0ed5-464d-80b3-315e80388c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def glu(x, v):\n",
    "    \"\"\"\n",
    "    Advantages:\n",
    "        Learnable Gating: GLU allows part of the network to act as a gate (through the sigmoid function), controlling the flow of information dynamically.\n",
    "        Parameter Efficiency: It introduces a gating mechanism, similar to LSTMs, without adding too much complexity or computation.\n",
    "    \"\"\"\n",
    "    return x * (1 / (1 + np.exp(-v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e93c3b4-6202-4d9c-937c-712317c86c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.62245933 1.63514895 2.64239123 2.09991675 3.65529289]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1, 2, 3, 4, 5])\n",
    "v = np.array([0.5, 1.5, 2.0, 0.1, 1.0])\n",
    "print(glu(x, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932272c9-9215-405b-a4c1-33303d18921b",
   "metadata": {},
   "source": [
    "# GELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36c9ac95-68c0-4631-9b9c-59a59dc354b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    \"\"\"\n",
    "    GELU overcomes the limitation of ReLU being non-differentiable at zero.\n",
    "    Advantages:\n",
    "        Smoothness: Unlike ReLU, which abruptly changes from 0 to x, GELU provides a smooth transition, which can lead to better optimization dynamics.\n",
    "        Probabilistic Interpretation: GELU can be interpreted as a probabilistic choice of keeping or discarding values based on their magnitude, which makes it more theoretically grounded than ReLU.\n",
    "    Disadvantages:\n",
    "        Computation: GELU is more computationally expensive compared to ReLU and SiLU, as it involves a Gaussian distribution or a tanh approximation.\n",
    "    \"\"\"\n",
    "    return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * np.power(x, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8e41c9f-3d78-4ad3-b42b-9119c8fe72b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.15880801  0.          0.84119199  1.95459769]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([-1, 0, 1, 2])\n",
    "print(gelu(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6602f38-e9f9-4b52-bbd5-9b018f80d726",
   "metadata": {},
   "source": [
    "# SiLU/Swish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7edef170-d2f2-487d-8832-b746515bd0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def silu(x, beta):\n",
    "    \"\"\"\n",
    "    Swish is differentiable at zero.\n",
    "    \n",
    "    \"\"\"\n",
    "    return x / (1 + np.exp(-beta * x))  # Same as Swish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b9e9054-063f-4300-911a-760268652e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.47502081  0.          0.52497919  1.09966799]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([-1, 0, 1, 2])\n",
    "print(silu(x, 0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3426c6ad-4a2e-4e84-a662-5c4577e5bf96",
   "metadata": {},
   "source": [
    "# SwiGLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "23bd4c75-8ae6-4317-a03c-0a3ef49d1c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def swiglu(x, v, beta):\n",
    "    # Swish applied to x and gate (sigmoid) applied to v\n",
    "    return (x / (1 + np.exp(-beta * x))) * (1 / (1 + np.exp(-v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f17ebb2b-c44f-46e8-9795-741bfe15b49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.32677819 0.89906048 1.51790187 1.25719425 2.27527117]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1, 2, 3, 4, 5])\n",
    "v = np.array([0.5, 1.5, 2.0, 0.1, 1.0])\n",
    "print(swiglu(x, v, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d96b0e-7787-4bc7-a772-8f9ebcb0ecd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
